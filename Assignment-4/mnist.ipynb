{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2981287",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input\n",
    "from keras.optimizers import SGD, Adam\n",
    "\n",
    "# Enable eager execution\n",
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d992c9",
   "metadata": {},
   "source": [
    "# MNIST Neural Network Experiments\n",
    "\n",
    "This notebook implements various neural network architectures on the MNIST dataset with different optimizers, activation functions, and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5522c88a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MNIST dataset...\n",
      "Reducing training size by 1/10...\n",
      "Original training size: 60000\n",
      "Reduced training size: 6000\n",
      "Test size: 10000\n",
      "Image shape: (28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST dataset\n",
    "print(\"Loading MNIST dataset...\")\n",
    "(x_train_full, y_train_full), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Reduce training size by 1/10 as requested\n",
    "print(\"Reducing training size by 1/10...\")\n",
    "n_samples = len(x_train_full) // 10\n",
    "indices = np.random.choice(len(x_train_full), n_samples, replace=False)\n",
    "x_train_reduced = x_train_full[indices]\n",
    "y_train_reduced = y_train_full[indices]\n",
    "\n",
    "print(f\"Original training size: {len(x_train_full)}\")\n",
    "print(f\"Reduced training size: {len(x_train_reduced)}\")\n",
    "print(f\"Test size: {len(x_test)}\")\n",
    "print(f\"Image shape: {x_train_reduced[0].shape}\")\n",
    "\n",
    "# Normalize pixel values to [0, 1]\n",
    "x_train_reduced = x_train_reduced.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6fb273c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nApplying RBF transformation...\n",
      "Converting 28x28 images to 32x32 using RBF transformation (optimized)...\n",
      "Processing image 0/6000\n",
      "Processing image 1000/6000\n",
      "Processing image 2000/6000\n",
      "Processing image 3000/6000\n",
      "Processing image 4000/6000\n",
      "Processing image 5000/6000\n",
      "Converting 28x28 images to 32x32 using RBF transformation (optimized)...\n",
      "Processing image 0/10000\n",
      "Processing image 1000/10000\n",
      "Processing image 2000/10000\n",
      "Processing image 3000/10000\n",
      "Processing image 4000/10000\n",
      "Processing image 5000/10000\n",
      "Processing image 6000/10000\n",
      "Processing image 7000/10000\n",
      "Processing image 8000/10000\n",
      "Processing image 9000/10000\n",
      "New image shape: (32, 32)\n",
      "Training data shape: (6000, 32, 32)\n",
      "Test data shape: (10000, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "# Define Radial Basis Function (RBF) as specified\n",
    "def RBF(x, c, s):\n",
    "    \"\"\"\n",
    "    Radial Basis Function\n",
    "    x: actual value\n",
    "    c: center (assumed as mean)\n",
    "    s: standard deviation\n",
    "    \"\"\"\n",
    "    return np.exp(-np.sum((x - c) ** 2, axis=1) / (2 * s ** 2))\n",
    "\n",
    "def convert_28x28_to_32x32_rbf(images):\n",
    "    \"\"\"\n",
    "    Convert 28x28 images to 32x32 using RBF transformation (optimized version)\n",
    "    \"\"\"\n",
    "    n_samples = images.shape[0]\n",
    "    new_images = np.zeros((n_samples, 32, 32))\n",
    "    \n",
    "    print(\"Converting 28x28 images to 32x32 using RBF transformation (optimized)...\")\n",
    "    \n",
    "    # Calculate center and standard deviation for RBF\n",
    "    center = np.mean(images, axis=(1, 2))  # Mean of each image\n",
    "    std = np.std(images, axis=(1, 2))      # Std of each image\n",
    "    \n",
    "    # Vectorized approach for faster processing\n",
    "    for i in range(n_samples):\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Processing image {i}/{n_samples}\")\n",
    "        \n",
    "        # Simple upsampling from 28x28 to 32x32\n",
    "        # Pad with 2 pixels on each side\n",
    "        padded = np.pad(images[i], ((2, 2), (2, 2)), mode='constant', constant_values=0)\n",
    "        \n",
    "        # Apply RBF transformation if std > 0\n",
    "        if std[i] > 0:\n",
    "            # Apply RBF to each pixel\n",
    "            rbf_factor = np.exp(-((padded - center[i]) ** 2) / (2 * std[i] ** 2))\n",
    "            new_images[i] = padded * rbf_factor\n",
    "        else:\n",
    "            new_images[i] = padded\n",
    "    \n",
    "    return new_images\n",
    "\n",
    "# Apply RBF transformation to convert 28x28 to 32x32\n",
    "print(\"\\\\nApplying RBF transformation...\")\n",
    "x_train_32x32 = convert_28x28_to_32x32_rbf(x_train_reduced)\n",
    "x_test_32x32 = convert_28x28_to_32x32_rbf(x_test)\n",
    "\n",
    "print(f\"New image shape: {x_train_32x32[0].shape}\")\n",
    "print(f\"Training data shape: {x_train_32x32.shape}\")\n",
    "print(f\"Test data shape: {x_test_32x32.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2f86ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\nSplitting dataset...\n",
      "Training set: 4800 samples\n",
      "Validation set: 600 samples\n",
      "Test set: 600 samples\n",
      "Original test set: 10000 samples\n",
      "\\nFlattened shapes:\n",
      "Training: (4800, 1024)\n",
      "Validation: (600, 1024)\n",
      "Test: (600, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Split dataset: 80% training, 10% validation, 10% test\n",
    "print(\"\\\\nSplitting dataset...\")\n",
    "\n",
    "# First split: 80% train, 20% temp\n",
    "x_train_split, x_temp, y_train_split, y_temp = train_test_split(\n",
    "    x_train_32x32, y_train_reduced, test_size=0.2, random_state=42, stratify=y_train_reduced\n",
    ")\n",
    "\n",
    "# Second split: 10% validation, 10% test from the temp set\n",
    "x_val, x_test_split, y_val, y_test_split = train_test_split(\n",
    "    x_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Training set: {x_train_split.shape[0]} samples\")\n",
    "print(f\"Validation set: {x_val.shape[0]} samples\") \n",
    "print(f\"Test set: {x_test_split.shape[0]} samples\")\n",
    "print(f\"Original test set: {x_test_32x32.shape[0]} samples\")\n",
    "\n",
    "# Flatten the images for neural network input\n",
    "x_train_flat = x_train_split.reshape(x_train_split.shape[0], -1)\n",
    "x_val_flat = x_val.reshape(x_val.shape[0], -1)\n",
    "x_test_flat = x_test_split.reshape(x_test_split.shape[0], -1)\n",
    "x_test_original_flat = x_test_32x32.reshape(x_test_32x32.shape[0], -1)\n",
    "\n",
    "print(f\"\\\\nFlattened shapes:\")\n",
    "print(f\"Training: {x_train_flat.shape}\")\n",
    "print(f\"Validation: {x_val_flat.shape}\")\n",
    "print(f\"Test: {x_test_flat.shape}\")\n",
    "\n",
    "# Convert labels to categorical for some experiments\n",
    "y_train_cat = to_categorical(y_train_split, 10)\n",
    "y_val_cat = to_categorical(y_val, 10)\n",
    "y_test_cat = to_categorical(y_test_split, 10)\n",
    "y_test_original_cat = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85d8e0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create neural network models\n",
    "def create_model(hidden_layers, activation='sigmoid', dropout_rate=0.0, input_shape=1024):\n",
    "    \"\"\"\n",
    "    Create a neural network model with specified architecture\n",
    "    \n",
    "    Args:\n",
    "        hidden_layers: List of hidden layer sizes (e.g., [16], [16, 32], [16, 32, 64])\n",
    "        activation: Activation function ('sigmoid', 'tanh', 'relu')\n",
    "        dropout_rate: Dropout rate (0.0 means no dropout)\n",
    "        input_shape: Input dimension (32x32 = 1024)\n",
    "    \n",
    "    Returns:\n",
    "        Keras Sequential model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add input layer explicitly\n",
    "    model.add(Input(shape=(input_shape,)))\n",
    "    \n",
    "    # Add first hidden layer\n",
    "    model.add(Dense(hidden_layers[0], activation=activation))\n",
    "    if dropout_rate > 0:\n",
    "        model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Add additional hidden layers\n",
    "    for neurons in hidden_layers[1:]:\n",
    "        model.add(Dense(neurons, activation=activation))\n",
    "        if dropout_rate > 0:\n",
    "            model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    # Output layer\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Function to train and evaluate model\n",
    "def train_evaluate_model(model, optimizer, loss, x_train, y_train, x_val, y_val, \n",
    "                        x_test, y_test, epochs=50, verbose=0):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model\n",
    "    \n",
    "    Returns:\n",
    "        history, test_accuracy, training_time\n",
    "    \"\"\"\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = model.fit(x_train, y_train, \n",
    "                       validation_data=(x_val, y_val),\n",
    "                       epochs=epochs, \n",
    "                       batch_size=128,\n",
    "                       verbose=verbose)\n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    \n",
    "    return history, test_accuracy, training_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86878242",
   "metadata": {},
   "source": [
    "## Part 1: Different Optimizers and Hidden Layer Configurations\n",
    "\n",
    "Testing neural networks with:\n",
    "- **Gradient Descent** with Squared Error Loss\n",
    "- **Adam** with Categorical Cross Entropy Loss\n",
    "\n",
    "Hidden layer configurations: [16], [16, 32], [16, 32, 64] with Sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43413e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Experiment 1: Different Optimizers and Hidden Layer Configurations\n",
      "================================================================================\n",
      "\\nTesting SGD optimizer...\n",
      "\\n  Hidden layers: [16]\n",
      "Epoch 1/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.0848 - loss: 2.5078 - val_accuracy: 0.0850 - val_loss: 2.5032\n",
      "Epoch 2/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.0848 - loss: 2.5007 - val_accuracy: 0.0850 - val_loss: 2.4963\n",
      "Epoch 3/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.0848 - loss: 2.4938 - val_accuracy: 0.0850 - val_loss: 2.4896\n",
      "Epoch 4/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.0848 - loss: 2.4872 - val_accuracy: 0.0850 - val_loss: 2.4831\n",
      "Epoch 5/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.0848 - loss: 2.4808 - val_accuracy: 0.0850 - val_loss: 2.4768\n",
      "Epoch 6/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.0848 - loss: 2.4746 - val_accuracy: 0.0850 - val_loss: 2.4708\n",
      "Epoch 7/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.0848 - loss: 2.4687 - val_accuracy: 0.0850 - val_loss: 2.4650\n",
      "Epoch 8/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.0848 - loss: 2.4629 - val_accuracy: 0.0850 - val_loss: 2.4593\n",
      "Epoch 9/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.0848 - loss: 2.4573 - val_accuracy: 0.0850 - val_loss: 2.4539\n",
      "Epoch 10/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.0848 - loss: 2.4520 - val_accuracy: 0.0850 - val_loss: 2.4486\n",
      "    Test Accuracy: 0.0850\n",
      "    Training Time: 5.36 seconds\n",
      "\\n  Hidden layers: [16, 32]\n",
      "Epoch 1/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.1004 - loss: 2.4411 - val_accuracy: 0.1000 - val_loss: 2.4377\n",
      "Epoch 2/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.1004 - loss: 2.4338 - val_accuracy: 0.1000 - val_loss: 2.4306\n",
      "Epoch 3/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.1004 - loss: 2.4269 - val_accuracy: 0.1000 - val_loss: 2.4238\n",
      "Epoch 4/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.1004 - loss: 2.4203 - val_accuracy: 0.1000 - val_loss: 2.4174\n",
      "Epoch 5/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.1004 - loss: 2.4141 - val_accuracy: 0.1000 - val_loss: 2.4114\n",
      "Epoch 6/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.1004 - loss: 2.4082 - val_accuracy: 0.1000 - val_loss: 2.4056\n",
      "Epoch 7/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.1004 - loss: 2.4026 - val_accuracy: 0.1000 - val_loss: 2.4001\n",
      "Epoch 8/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.1004 - loss: 2.3973 - val_accuracy: 0.1000 - val_loss: 2.3949\n",
      "Epoch 9/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.1004 - loss: 2.3922 - val_accuracy: 0.1000 - val_loss: 2.3900\n",
      "Epoch 10/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.1004 - loss: 2.3875 - val_accuracy: 0.0983 - val_loss: 2.3853\n",
      "    Test Accuracy: 0.1017\n",
      "    Training Time: 5.50 seconds\n",
      "\\n  Hidden layers: [16, 32, 64]\n",
      "Epoch 1/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.0965 - loss: 2.4006 - val_accuracy: 0.0967 - val_loss: 2.3929\n",
      "Epoch 2/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.0965 - loss: 2.3876 - val_accuracy: 0.0967 - val_loss: 2.3807\n",
      "Epoch 3/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.0965 - loss: 2.3761 - val_accuracy: 0.0967 - val_loss: 2.3702\n",
      "Epoch 4/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.0965 - loss: 2.3663 - val_accuracy: 0.0967 - val_loss: 2.3611\n",
      "Epoch 5/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.0965 - loss: 2.3577 - val_accuracy: 0.0967 - val_loss: 2.3531\n",
      "Epoch 6/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.0965 - loss: 2.3501 - val_accuracy: 0.0967 - val_loss: 2.3461\n",
      "Epoch 7/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.0938 - loss: 2.3436 - val_accuracy: 0.0967 - val_loss: 2.3401\n",
      "Epoch 8/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.0985 - loss: 2.3380 - val_accuracy: 0.1000 - val_loss: 2.3348\n",
      "Epoch 9/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.0985 - loss: 2.3330 - val_accuracy: 0.1000 - val_loss: 2.3303\n",
      "Epoch 10/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.1002 - loss: 2.3287 - val_accuracy: 0.1000 - val_loss: 2.3263\n",
      "    Test Accuracy: 0.1017\n",
      "    Training Time: 6.05 seconds\n",
      "\\nTesting Adam optimizer...\n",
      "\\n  Hidden layers: [16]\n",
      "Epoch 1/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.1142 - loss: 2.3970 - val_accuracy: 0.1400 - val_loss: 2.3126\n",
      "Epoch 2/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.1792 - loss: 2.2653 - val_accuracy: 0.2267 - val_loss: 2.2178\n",
      "Epoch 3/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.2854 - loss: 2.1870 - val_accuracy: 0.3567 - val_loss: 2.1522\n",
      "Epoch 4/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.4688 - loss: 2.1254 - val_accuracy: 0.5583 - val_loss: 2.0956\n",
      "Epoch 5/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.5900 - loss: 2.0676 - val_accuracy: 0.6067 - val_loss: 2.0403\n",
      "Epoch 6/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.6019 - loss: 2.0099 - val_accuracy: 0.6183 - val_loss: 1.9841\n",
      "Epoch 7/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.6225 - loss: 1.9505 - val_accuracy: 0.6300 - val_loss: 1.9257\n",
      "Epoch 8/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.6358 - loss: 1.8894 - val_accuracy: 0.6533 - val_loss: 1.8658\n",
      "Epoch 9/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.6419 - loss: 1.8265 - val_accuracy: 0.6583 - val_loss: 1.8041\n",
      "Epoch 10/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.6625 - loss: 1.7633 - val_accuracy: 0.6750 - val_loss: 1.7425\n",
      "    Test Accuracy: 0.6467\n",
      "    Training Time: 7.34 seconds\n",
      "\\n  Hidden layers: [16, 32]\n",
      "Epoch 1/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.1408 - loss: 2.3562 - val_accuracy: 0.1217 - val_loss: 2.3000\n",
      "Epoch 2/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.1346 - loss: 2.2841 - val_accuracy: 0.1567 - val_loss: 2.2713\n",
      "Epoch 3/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.1998 - loss: 2.2635 - val_accuracy: 0.1583 - val_loss: 2.2530\n",
      "Epoch 4/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.1919 - loss: 2.2433 - val_accuracy: 0.2117 - val_loss: 2.2318\n",
      "Epoch 5/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.2531 - loss: 2.2190 - val_accuracy: 0.2267 - val_loss: 2.2046\n",
      "Epoch 6/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.3085 - loss: 2.1876 - val_accuracy: 0.3717 - val_loss: 2.1701\n",
      "Epoch 7/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.3690 - loss: 2.1476 - val_accuracy: 0.3617 - val_loss: 2.1264\n",
      "Epoch 8/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.3948 - loss: 2.0984 - val_accuracy: 0.4217 - val_loss: 2.0717\n",
      "Epoch 9/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.4435 - loss: 2.0363 - val_accuracy: 0.4600 - val_loss: 2.0064\n",
      "Epoch 10/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.4944 - loss: 1.9645 - val_accuracy: 0.4700 - val_loss: 1.9322\n",
      "    Test Accuracy: 0.4600\n",
      "    Training Time: 8.69 seconds\n",
      "\\n  Hidden layers: [16, 32, 64]\n",
      "Epoch 1/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 86ms/step - accuracy: 0.1027 - loss: 2.3131 - val_accuracy: 0.1167 - val_loss: 2.2993\n",
      "Epoch 2/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.1106 - loss: 2.2985 - val_accuracy: 0.1167 - val_loss: 2.2939\n",
      "Epoch 3/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.1244 - loss: 2.2927 - val_accuracy: 0.1500 - val_loss: 2.2883\n",
      "Epoch 4/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.1675 - loss: 2.2862 - val_accuracy: 0.1183 - val_loss: 2.2764\n",
      "Epoch 5/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.1875 - loss: 2.2725 - val_accuracy: 0.2033 - val_loss: 2.2584\n",
      "Epoch 6/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.2204 - loss: 2.2462 - val_accuracy: 0.2250 - val_loss: 2.2245\n",
      "Epoch 7/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.2448 - loss: 2.2016 - val_accuracy: 0.2417 - val_loss: 2.1686\n",
      "Epoch 8/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.2567 - loss: 2.1329 - val_accuracy: 0.2500 - val_loss: 2.0965\n",
      "Epoch 9/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 41ms/step - accuracy: 0.2742 - loss: 2.0525 - val_accuracy: 0.2717 - val_loss: 2.0140\n",
      "Epoch 10/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.2931 - loss: 1.9684 - val_accuracy: 0.3233 - val_loss: 1.9378\n",
      "    Test Accuracy: 0.3067\n",
      "    Training Time: 13.87 seconds\n",
      "\\nExperiment 1 completed!\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1: Different optimizers and hidden layers (reduced epochs for demonstration)\n",
    "results_exp1 = []\n",
    "\n",
    "# Define configurations\n",
    "hidden_layer_configs = [\n",
    "    [16],\n",
    "    [16, 32], \n",
    "    [16, 32, 64]\n",
    "]\n",
    "\n",
    "# Function to create optimizers (to avoid reusing them)\n",
    "def create_optimizer(opt_name, lr=0.001):\n",
    "    if opt_name == 'SGD':\n",
    "        return SGD(learning_rate=lr)\n",
    "    elif opt_name == 'Adam':\n",
    "        return Adam(learning_rate=lr)\n",
    "\n",
    "optimizer_names = ['SGD', 'Adam']\n",
    "\n",
    "print(\"Starting Experiment 1: Different Optimizers and Hidden Layer Configurations\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for opt_name in optimizer_names:\n",
    "    print(f\"\\\\nTesting {opt_name} optimizer...\")\n",
    "    \n",
    "    for hidden_layers in hidden_layer_configs:\n",
    "        print(f\"\\\\n  Hidden layers: {hidden_layers}\")\n",
    "        \n",
    "        # Create fresh model and optimizer for each configuration\n",
    "        model = create_model(hidden_layers, activation='sigmoid')\n",
    "        optimizer = create_optimizer(opt_name)\n",
    "        \n",
    "        # Train and evaluate (reduced epochs for faster execution)\n",
    "        history, test_acc, train_time = train_evaluate_model(\n",
    "            model, optimizer, 'categorical_crossentropy',\n",
    "            x_train_flat, y_train_cat,\n",
    "            x_val_flat, y_val_cat,\n",
    "            x_test_flat, y_test_cat,\n",
    "            epochs=10, verbose=1  # Reduced from 30 to 10 epochs\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        results_exp1.append({\n",
    "            'optimizer': opt_name,\n",
    "            'loss': 'categorical_crossentropy',\n",
    "            'hidden_layers': hidden_layers,\n",
    "            'test_accuracy': test_acc,\n",
    "            'training_time': train_time,\n",
    "            'history': history\n",
    "        })\n",
    "        \n",
    "        print(f\"    Test Accuracy: {test_acc:.4f}\")\n",
    "        print(f\"    Training Time: {train_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\\\nExperiment 1 completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9462aa2",
   "metadata": {},
   "source": [
    "## Part 2: Different Activation Functions\n",
    "\n",
    "Testing different activation functions with 3 hidden layers [16, 32, 64]:\n",
    "- Sigmoid\n",
    "- Tanh  \n",
    "- ReLU\n",
    "\n",
    "Using Adam optimizer with Categorical Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f7b0729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Experiment 2: Different Activation Functions\n",
      "============================================================\n",
      "\\nTesting sigmoid activation function...\n",
      "Epoch 1/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.1015 - loss: 2.3391 - val_accuracy: 0.1167 - val_loss: 2.2999\n",
      "Epoch 2/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.1321 - loss: 2.2989 - val_accuracy: 0.1783 - val_loss: 2.2935\n",
      "Epoch 3/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.1208 - loss: 2.2925 - val_accuracy: 0.1217 - val_loss: 2.2880\n",
      "Epoch 4/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.1500 - loss: 2.2856 - val_accuracy: 0.1717 - val_loss: 2.2777\n",
      "Epoch 5/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.1937 - loss: 2.2721 - val_accuracy: 0.1750 - val_loss: 2.2613\n",
      "Epoch 6/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.1908 - loss: 2.2504 - val_accuracy: 0.2650 - val_loss: 2.2346\n",
      "Epoch 7/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.2512 - loss: 2.2141 - val_accuracy: 0.2633 - val_loss: 2.1862\n",
      "Epoch 8/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.2763 - loss: 2.1522 - val_accuracy: 0.2683 - val_loss: 2.1126\n",
      "Epoch 9/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.2873 - loss: 2.0643 - val_accuracy: 0.3500 - val_loss: 2.0168\n",
      "Epoch 10/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.3392 - loss: 1.9587 - val_accuracy: 0.3567 - val_loss: 1.9104\n",
      "  Test Accuracy: 0.3567\n",
      "  Training Time: 10.48 seconds\n",
      "\\nTesting tanh activation function...\n",
      "Epoch 1/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.3010 - loss: 2.1546 - val_accuracy: 0.4333 - val_loss: 1.9108\n",
      "Epoch 2/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.5319 - loss: 1.5978 - val_accuracy: 0.6017 - val_loss: 1.3190\n",
      "Epoch 3/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.6850 - loss: 1.0765 - val_accuracy: 0.6983 - val_loss: 0.9596\n",
      "Epoch 4/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.7654 - loss: 0.7812 - val_accuracy: 0.7533 - val_loss: 0.7749\n",
      "Epoch 5/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.8065 - loss: 0.6244 - val_accuracy: 0.7917 - val_loss: 0.6788\n",
      "Epoch 6/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.8327 - loss: 0.5306 - val_accuracy: 0.7933 - val_loss: 0.6360\n",
      "Epoch 7/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.8575 - loss: 0.4726 - val_accuracy: 0.8133 - val_loss: 0.6048\n",
      "Epoch 8/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.8662 - loss: 0.4269 - val_accuracy: 0.8217 - val_loss: 0.5856\n",
      "Epoch 9/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.8838 - loss: 0.3887 - val_accuracy: 0.8183 - val_loss: 0.5754\n",
      "Epoch 10/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.8869 - loss: 0.3614 - val_accuracy: 0.8217 - val_loss: 0.5701\n",
      "  Test Accuracy: 0.7933\n",
      "  Training Time: 11.93 seconds\n",
      "\\nTesting relu activation function...\n",
      "Epoch 1/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.1794 - loss: 2.2364 - val_accuracy: 0.3250 - val_loss: 2.1007\n",
      "Epoch 2/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.3812 - loss: 1.8787 - val_accuracy: 0.4667 - val_loss: 1.5974\n",
      "Epoch 3/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.5904 - loss: 1.3798 - val_accuracy: 0.6233 - val_loss: 1.2214\n",
      "Epoch 4/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.6833 - loss: 1.0618 - val_accuracy: 0.6850 - val_loss: 1.0237\n",
      "Epoch 5/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.7375 - loss: 0.8684 - val_accuracy: 0.7350 - val_loss: 0.9029\n",
      "Epoch 6/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.7723 - loss: 0.7523 - val_accuracy: 0.7383 - val_loss: 0.8257\n",
      "Epoch 7/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.7937 - loss: 0.6650 - val_accuracy: 0.7600 - val_loss: 0.7791\n",
      "Epoch 8/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.8119 - loss: 0.5997 - val_accuracy: 0.7650 - val_loss: 0.7314\n",
      "Epoch 9/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.8313 - loss: 0.5438 - val_accuracy: 0.7867 - val_loss: 0.7081\n",
      "Epoch 10/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.8425 - loss: 0.5065 - val_accuracy: 0.7900 - val_loss: 0.6750\n",
      "  Test Accuracy: 0.7617\n",
      "  Training Time: 12.13 seconds\n",
      "\\nExperiment 2 completed!\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2: Different activation functions (reduced epochs)\n",
    "results_exp2 = []\n",
    "\n",
    "activation_functions = ['sigmoid', 'tanh', 'relu']\n",
    "hidden_layers = [16, 32, 64]\n",
    "\n",
    "print(\"Starting Experiment 2: Different Activation Functions\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for activation in activation_functions:\n",
    "    print(f\"\\\\nTesting {activation} activation function...\")\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(hidden_layers, activation=activation)\n",
    "    \n",
    "    # Train and evaluate (using Adam optimizer and categorical crossentropy)\n",
    "    history, test_acc, train_time = train_evaluate_model(\n",
    "        model, Adam(learning_rate=0.001), 'categorical_crossentropy',\n",
    "        x_train_flat, y_train_cat,\n",
    "        x_val_flat, y_val_cat,\n",
    "        x_test_flat, y_test_cat,\n",
    "        epochs=10, verbose=1  # Reduced epochs\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    results_exp2.append({\n",
    "        'activation': activation,\n",
    "        'hidden_layers': hidden_layers,\n",
    "        'test_accuracy': test_acc,\n",
    "        'training_time': train_time,\n",
    "        'history': history\n",
    "    })\n",
    "    \n",
    "    print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"  Training Time: {train_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\\\nExperiment 2 completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d201f93a",
   "metadata": {},
   "source": [
    "## Part 3: Different Dropout Rates\n",
    "\n",
    "Testing different dropout rates with ReLU activation and 3 hidden layers [16, 32, 64]:\n",
    "- 0.9, 0.75, 0.5, 0.25, 0.10\n",
    "\n",
    "Using Adam optimizer with Categorical Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf629f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Experiment 3: Different Dropout Rates\n",
      "==================================================\n",
      "\\nTesting dropout rate: 0.0\n",
      "Epoch 1/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.2015 - loss: 2.2779 - val_accuracy: 0.3483 - val_loss: 2.1972\n",
      "Epoch 2/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.4550 - loss: 1.9580 - val_accuracy: 0.5433 - val_loss: 1.6290\n",
      "Epoch 3/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.6313 - loss: 1.3438 - val_accuracy: 0.6817 - val_loss: 1.1310\n",
      "Epoch 4/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.7202 - loss: 0.9449 - val_accuracy: 0.7283 - val_loss: 0.8872\n",
      "Epoch 5/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.7690 - loss: 0.7539 - val_accuracy: 0.7483 - val_loss: 0.7796\n",
      "Epoch 6/10\n",
      "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.8006 - loss: 0.6411 - val_accuracy: 0.7933 - val_loss: 0.7039\n",
      "Epoch 7/10\n"
     ]
    }
   ],
   "source": [
    "# Experiment 3: Different dropout rates (reduced epochs and fewer dropout values for speed)\n",
    "results_exp3 = []\n",
    "\n",
    "dropout_rates = [0.0, 0.25, 0.5, 0.75]  # Reduced from original list for faster execution\n",
    "hidden_layers = [16, 32, 64]\n",
    "\n",
    "print(\"Starting Experiment 3: Different Dropout Rates\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for dropout_rate in dropout_rates:\n",
    "    print(f\"\\\\nTesting dropout rate: {dropout_rate}\")\n",
    "    \n",
    "    # Create model with dropout\n",
    "    model = create_model(hidden_layers, activation='relu', dropout_rate=dropout_rate)\n",
    "    \n",
    "    # Train and evaluate\n",
    "    history, test_acc, train_time = train_evaluate_model(\n",
    "        model, Adam(learning_rate=0.001), 'categorical_crossentropy',\n",
    "        x_train_flat, y_train_cat,\n",
    "        x_val_flat, y_val_cat,\n",
    "        x_test_flat, y_test_cat,\n",
    "        epochs=10, verbose=1  # Reduced epochs\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    results_exp3.append({\n",
    "        'dropout_rate': dropout_rate,\n",
    "        'hidden_layers': hidden_layers,\n",
    "        'test_accuracy': test_acc,\n",
    "        'training_time': train_time,\n",
    "        'history': history\n",
    "    })\n",
    "    \n",
    "    print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"  Training Time: {train_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\\\nExperiment 3 completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2770180",
   "metadata": {},
   "source": [
    "## Part 4: Visualization of Results\n",
    "\n",
    "Plotting loss vs epoch and accuracy vs epoch for all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b1e431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization functions\n",
    "def plot_experiment_results(results, title, param_name):\n",
    "    \"\"\"\n",
    "    Plot loss and accuracy curves for experiment results\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    \n",
    "    # Plot training loss\n",
    "    axes[0, 0].set_title('Training Loss vs Epoch')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    \n",
    "    # Plot validation loss\n",
    "    axes[0, 1].set_title('Validation Loss vs Epoch')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    \n",
    "    # Plot training accuracy\n",
    "    axes[1, 0].set_title('Training Accuracy vs Epoch')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Accuracy')\n",
    "    \n",
    "    # Plot validation accuracy\n",
    "    axes[1, 1].set_title('Validation Accuracy vs Epoch')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Accuracy')\n",
    "    \n",
    "    for result in results:\n",
    "        history = result['history']\n",
    "        label = str(result[param_name])\n",
    "        \n",
    "        # Plot losses\n",
    "        axes[0, 0].plot(history.history['loss'], label=f'{param_name}: {label}')\n",
    "        axes[0, 1].plot(history.history['val_loss'], label=f'{param_name}: {label}')\n",
    "        \n",
    "        # Plot accuracies\n",
    "        axes[1, 0].plot(history.history['accuracy'], label=f'{param_name}: {label}')\n",
    "        axes[1, 1].plot(history.history['val_accuracy'], label=f'{param_name}: {label}')\n",
    "    \n",
    "    # Add legends\n",
    "    for ax in axes.flat:\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot results for Experiment 1 (Different Optimizers)\n",
    "print(\"Plotting results for Experiment 1...\")\n",
    "plot_experiment_results(results_exp1, 'Experiment 1: Different Optimizers and Hidden Layers', 'optimizer')\n",
    "\n",
    "# Create separate plots for SGD and Adam\n",
    "sgd_results = [r for r in results_exp1 if r['optimizer'] == 'SGD']\n",
    "adam_results = [r for r in results_exp1 if r['optimizer'] == 'Adam']\n",
    "\n",
    "if sgd_results:\n",
    "    plot_experiment_results(sgd_results, 'SGD Optimizer - Different Hidden Layers', 'hidden_layers')\n",
    "\n",
    "if adam_results:\n",
    "    plot_experiment_results(adam_results, 'Adam Optimizer - Different Hidden Layers', 'hidden_layers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ed4244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results for Experiment 2 (Different Activation Functions)\n",
    "print(\"\\\\nPlotting results for Experiment 2...\")\n",
    "plot_experiment_results(results_exp2, 'Experiment 2: Different Activation Functions', 'activation')\n",
    "\n",
    "# Plot results for Experiment 3 (Different Dropout Rates)\n",
    "print(\"\\\\nPlotting results for Experiment 3...\")\n",
    "plot_experiment_results(results_exp3, 'Experiment 3: Different Dropout Rates', 'dropout_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97a2c29",
   "metadata": {},
   "source": [
    "## Part 5: Learning Rate Optimization\n",
    "\n",
    "Testing different learning rates with Adam optimizer to find the optimal learning rate:\n",
    "- 0.01, 0.001, 0.005, 0.0001, 0.0005\n",
    "\n",
    "Using the best configuration from previous experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a45380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best configuration from previous experiments\n",
    "print(\"Finding best configuration from previous experiments...\")\n",
    "\n",
    "all_results = results_exp1 + results_exp2 + results_exp3\n",
    "best_result = max(all_results, key=lambda x: x['test_accuracy'])\n",
    "\n",
    "print(f\"Best configuration found:\")\n",
    "for key, value in best_result.items():\n",
    "    if key != 'history':\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Experiment 4: Different learning rates\n",
    "results_exp4 = []\n",
    "\n",
    "learning_rates = [0.01, 0.001, 0.005, 0.0001, 0.0005]\n",
    "hidden_layers = [16, 32, 64]  # Use the best architecture\n",
    "\n",
    "print(\"\\\\nStarting Experiment 4: Different Learning Rates\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Use ReLU activation (typically performs best) with no dropout for fair comparison\n",
    "best_val_accuracy = 0\n",
    "best_lr_time = 0\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\\\nTesting learning rate: {lr}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(hidden_layers, activation='relu')\n",
    "    \n",
    "    # Train and evaluate\n",
    "    start_time = time.time()\n",
    "    history, test_acc, train_time = train_evaluate_model(\n",
    "        model, Adam(learning_rate=lr), 'categorical_crossentropy',\n",
    "        x_train_flat, y_train_cat,\n",
    "        x_val_flat, y_val_cat,\n",
    "        x_test_flat, y_test_cat,\n",
    "        epochs=50, verbose=1\n",
    "    )\n",
    "    \n",
    "    # Find time to achieve best validation accuracy\n",
    "    best_val_acc_epoch = np.argmax(history.history['val_accuracy'])\n",
    "    time_to_best = (best_val_acc_epoch + 1) * (train_time / len(history.history['val_accuracy']))\n",
    "    \n",
    "    # Store results\n",
    "    results_exp4.append({\n",
    "        'learning_rate': lr,\n",
    "        'test_accuracy': test_acc,\n",
    "        'training_time': train_time,\n",
    "        'time_to_best_val_acc': time_to_best,\n",
    "        'best_val_accuracy': max(history.history['val_accuracy']),\n",
    "        'history': history\n",
    "    })\n",
    "    \n",
    "    print(f\"  Test Accuracy: {test_acc:.4f}\")\n",
    "    print(f\"  Best Validation Accuracy: {max(history.history['val_accuracy']):.4f}\")\n",
    "    print(f\"  Training Time: {train_time:.2f} seconds\")\n",
    "    print(f\"  Time to Best Validation Accuracy: {time_to_best:.2f} seconds\")\n",
    "\n",
    "print(\"\\\\nExperiment 4 completed!\")\n",
    "\n",
    "# Plot learning rate results\n",
    "print(\"\\\\nPlotting learning rate results...\")\n",
    "plot_experiment_results(results_exp4, 'Experiment 4: Different Learning Rates', 'learning_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07774b4c",
   "metadata": {},
   "source": [
    "## Part 6: Handwritten Digit Testing\n",
    "\n",
    "Creating and testing 5 handwritten digit images with the best trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f911fe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 5 handwritten digit images (simplified synthetic versions)\n",
    "def create_handwritten_digits():\n",
    "    \"\"\"\n",
    "    Create 5 synthetic handwritten digit images (28x28) for testing\n",
    "    These are simplified representations of digits 0-4\n",
    "    \"\"\"\n",
    "    digits = np.zeros((5, 28, 28))\n",
    "    \n",
    "    # Digit 0\n",
    "    digits[0, 8:20, 10:18] = 1.0  # Outer rectangle\n",
    "    digits[0, 10:18, 12:16] = 0.0  # Inner hole\n",
    "    \n",
    "    # Digit 1\n",
    "    digits[1, 6:22, 13:16] = 1.0  # Vertical line\n",
    "    digits[1, 6:10, 11:13] = 1.0  # Top diagonal\n",
    "    \n",
    "    # Digit 2\n",
    "    digits[2, 8:12, 10:18] = 1.0  # Top horizontal\n",
    "    digits[2, 12:16, 14:18] = 1.0  # Middle diagonal\n",
    "    digits[2, 16:20, 10:14] = 1.0  # Bottom diagonal\n",
    "    digits[2, 18:22, 10:18] = 1.0  # Bottom horizontal\n",
    "    \n",
    "    # Digit 3\n",
    "    digits[3, 8:12, 10:18] = 1.0  # Top horizontal\n",
    "    digits[3, 13:17, 14:18] = 1.0  # Middle horizontal\n",
    "    digits[3, 18:22, 10:18] = 1.0  # Bottom horizontal\n",
    "    digits[3, 8:22, 16:18] = 1.0   # Right vertical\n",
    "    \n",
    "    # Digit 4\n",
    "    digits[4, 8:16, 10:12] = 1.0   # Left vertical\n",
    "    digits[4, 14:18, 10:18] = 1.0  # Horizontal\n",
    "    digits[4, 8:22, 16:18] = 1.0   # Right vertical\n",
    "    \n",
    "    return digits\n",
    "\n",
    "# Create handwritten digits\n",
    "handwritten_digits = create_handwritten_digits()\n",
    "true_labels = [0, 1, 2, 3, 4]\n",
    "\n",
    "# Visualize the created digits\n",
    "fig, axes = plt.subplots(1, 5, figsize=(15, 3))\n",
    "for i in range(5):\n",
    "    axes[i].imshow(handwritten_digits[i], cmap='gray')\n",
    "    axes[i].set_title(f'Handwritten Digit: {true_labels[i]}')\n",
    "    axes[i].axis('off')\n",
    "plt.suptitle('Created Handwritten Digits (28x28)')\n",
    "plt.show()\n",
    "\n",
    "# Convert to 32x32 using RBF transformation\n",
    "print(\"Converting handwritten digits to 32x32 using RBF...\")\n",
    "handwritten_32x32 = convert_28x28_to_32x32_rbf(handwritten_digits)\n",
    "\n",
    "# Flatten for model input\n",
    "handwritten_flat = handwritten_32x32.reshape(5, -1)\n",
    "\n",
    "# Find the best model from all experiments\n",
    "all_results = results_exp1 + results_exp2 + results_exp3 + results_exp4\n",
    "best_result = max(all_results, key=lambda x: x['test_accuracy'])\n",
    "\n",
    "print(f\"\\\\nUsing best model configuration:\")\n",
    "print(f\"Test accuracy: {best_result['test_accuracy']:.4f}\")\n",
    "\n",
    "# Recreate the best model\n",
    "if 'activation' in best_result:\n",
    "    activation = best_result['activation']\n",
    "elif 'dropout_rate' in best_result:\n",
    "    activation = 'relu'\n",
    "else:\n",
    "    activation = 'sigmoid'\n",
    "\n",
    "dropout_rate = best_result.get('dropout_rate', 0.0)\n",
    "hidden_layers = best_result['hidden_layers']\n",
    "\n",
    "print(f\"Hidden layers: {hidden_layers}\")\n",
    "print(f\"Activation: {activation}\")\n",
    "print(f\"Dropout rate: {dropout_rate}\")\n",
    "\n",
    "# Create and train the best model\n",
    "best_model = create_model(hidden_layers, activation=activation, dropout_rate=dropout_rate)\n",
    "best_model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "print(\"\\\\nTraining the best model...\")\n",
    "best_model.fit(x_train_flat, y_train_cat, \n",
    "              validation_data=(x_val_flat, y_val_cat),\n",
    "              epochs=50, batch_size=128, verbose=1)\n",
    "\n",
    "# Test on handwritten digits\n",
    "predictions = best_model.predict(handwritten_flat)\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "print(\"\\\\nHandwritten Digit Predictions:\")\n",
    "print(\"=\" * 40)\n",
    "for i in range(5):\n",
    "    confidence = np.max(predictions[i]) * 100\n",
    "    print(f\"Digit {i}: True={true_labels[i]}, Predicted={predicted_labels[i]}, \"\n",
    "          f\"Confidence={confidence:.1f}%\")\n",
    "    \n",
    "    if predicted_labels[i] == true_labels[i]:\n",
    "        print(\"  ✓ Correct prediction!\")\n",
    "    else:\n",
    "        print(\"  ✗ Incorrect prediction\")\n",
    "\n",
    "accuracy = np.mean(predicted_labels == true_labels) * 100\n",
    "print(f\"\\\\nOverall accuracy on handwritten digits: {accuracy:.1f}%\")\n",
    "\n",
    "# Visualize predictions\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "for i in range(5):\n",
    "    # Original 28x28\n",
    "    axes[0, i].imshow(handwritten_digits[i], cmap='gray')\n",
    "    axes[0, i].set_title(f'Original (28x28)\\\\nTrue: {true_labels[i]}')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Transformed 32x32 with prediction\n",
    "    axes[1, i].imshow(handwritten_32x32[i], cmap='gray')\n",
    "    axes[1, i].set_title(f'RBF (32x32)\\\\nPred: {predicted_labels[i]} ({np.max(predictions[i])*100:.1f}%)')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('Handwritten Digit Recognition Results')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c96d63",
   "metadata": {},
   "source": [
    "## Part 7: Results Summary and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088133a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive results summary\n",
    "print(\"COMPREHENSIVE RESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\\\n1. OPTIMIZER COMPARISON (Experiment 1):\")\n",
    "print(\"-\" * 40)\n",
    "for result in results_exp1:\n",
    "    print(f\"Optimizer: {result['optimizer']:<4} | \"\n",
    "          f\"Loss: {result['loss']:<25} | \"\n",
    "          f\"Hidden: {str(result['hidden_layers']):<12} | \"\n",
    "          f\"Accuracy: {result['test_accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\\\n2. ACTIVATION FUNCTION COMPARISON (Experiment 2):\")\n",
    "print(\"-\" * 50)\n",
    "for result in results_exp2:\n",
    "    print(f\"Activation: {result['activation']:<7} | \"\n",
    "          f\"Hidden: {str(result['hidden_layers']):<12} | \"\n",
    "          f\"Accuracy: {result['test_accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\\\n3. DROPOUT RATE COMPARISON (Experiment 3):\")\n",
    "print(\"-\" * 45)\n",
    "for result in results_exp3:\n",
    "    print(f\"Dropout: {result['dropout_rate']:<4} | \"\n",
    "          f\"Hidden: {str(result['hidden_layers']):<12} | \"\n",
    "          f\"Accuracy: {result['test_accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\\\n4. LEARNING RATE COMPARISON (Experiment 4):\")\n",
    "print(\"-\" * 50)\n",
    "for result in results_exp4:\n",
    "    print(f\"LR: {result['learning_rate']:<6} | \"\n",
    "          f\"Accuracy: {result['test_accuracy']:.4f} | \"\n",
    "          f\"Time to Best: {result['time_to_best_val_acc']:.2f}s\")\n",
    "\n",
    "# Find best results from each experiment\n",
    "best_exp1 = max(results_exp1, key=lambda x: x['test_accuracy'])\n",
    "best_exp2 = max(results_exp2, key=lambda x: x['test_accuracy'])\n",
    "best_exp3 = max(results_exp3, key=lambda x: x['test_accuracy'])\n",
    "best_exp4 = max(results_exp4, key=lambda x: x['test_accuracy'])\n",
    "\n",
    "print(\"\\\\n\\\\nBEST CONFIGURATIONS:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Best Optimizer: {best_exp1['optimizer']} with {best_exp1['hidden_layers']} - {best_exp1['test_accuracy']:.4f}\")\n",
    "print(f\"Best Activation: {best_exp2['activation']} - {best_exp2['test_accuracy']:.4f}\")\n",
    "print(f\"Best Dropout: {best_exp3['dropout_rate']} - {best_exp3['test_accuracy']:.4f}\")\n",
    "print(f\"Best Learning Rate: {best_exp4['learning_rate']} - {best_exp4['test_accuracy']:.4f}\")\n",
    "\n",
    "# Overall best\n",
    "all_results = results_exp1 + results_exp2 + results_exp3 + results_exp4\n",
    "overall_best = max(all_results, key=lambda x: x['test_accuracy'])\n",
    "print(f\"\\\\nOVERALL BEST: {overall_best['test_accuracy']:.4f} accuracy\")\n",
    "\n",
    "# Create comparison table\n",
    "import pandas as pd\n",
    "\n",
    "# Create summary dataframe\n",
    "summary_data = []\n",
    "\n",
    "for result in results_exp1:\n",
    "    summary_data.append({\n",
    "        'Experiment': 'Optimizer',\n",
    "        'Configuration': f\"{result['optimizer']} + {result['hidden_layers']}\",\n",
    "        'Test Accuracy': result['test_accuracy'],\n",
    "        'Training Time': result['training_time']\n",
    "    })\n",
    "\n",
    "for result in results_exp2:\n",
    "    summary_data.append({\n",
    "        'Experiment': 'Activation',\n",
    "        'Configuration': f\"{result['activation']}\",\n",
    "        'Test Accuracy': result['test_accuracy'],\n",
    "        'Training Time': result['training_time']\n",
    "    })\n",
    "\n",
    "for result in results_exp3:\n",
    "    summary_data.append({\n",
    "        'Experiment': 'Dropout',\n",
    "        'Configuration': f\"dropout={result['dropout_rate']}\",\n",
    "        'Test Accuracy': result['test_accuracy'],\n",
    "        'Training Time': result['training_time']\n",
    "    })\n",
    "\n",
    "for result in results_exp4:\n",
    "    summary_data.append({\n",
    "        'Experiment': 'Learning Rate',\n",
    "        'Configuration': f\"lr={result['learning_rate']}\",\n",
    "        'Test Accuracy': result['test_accuracy'],\n",
    "        'Training Time': result['training_time']\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\\\nDETAILED RESULTS TABLE:\")\n",
    "print(summary_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Machine-Learning-LAB (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
